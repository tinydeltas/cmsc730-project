# -*- coding: utf-8 -*-
"""GestureRecognitionCNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QoKRASpSKjJhS9m6TMOWDRVC8CmKtlia
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from scipy.io import wavfile
import glob as glob
import soundfile as sf
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam

#%cd "drive/MyDrive/Colab Notebooks/CMSC730Project/"
glob.glob("*")

def one_hot_encode(x, n_classes):
    """
    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.
    : x: List of sample Labels
    : return: Numpy array of one-hot encoded labels
     """
    return np.eye(n_classes)[x]

DataFolder = "LatestData/"
ListFiles = glob.glob(DataFolder+"*.wav")


print(len(ListFiles))

ListFiles[0][11:-3]

for i in range(len(ListFiles)):
  i
  data, samplerate = sf.read(ListFiles[i])
  Pxx, freqs, bins, im = plt.specgram(data, NFFT=4800, Fs=samplerate,  window=None  , noverlap=4560)
  Spec = 20*np.log10(Pxx[0:800,:])
  np.save("LatestTraining/"+ListFiles[i][11:-3]+".npy", Spec)

# loading data
def load_data(datadir):
  Data = []
  Label = []
  LabelList = ["close", "cut", "no", "open", "switch", "paste", "maximize", "minimize"]
  ListFiles = glob.glob(datadir + "*.npy")
  for i in range(len(ListFiles)):
    Data.append(np.load(ListFiles[i]))
    temp = ListFiles[i].split("_")[0]
    labelname = temp.split("/")[1]
    Label.append(LabelList.index(labelname))
  Data = np.array(Data)
  Label = np.array(Label)
  
  return Data, Label

#LabelList = ["close", "cut", "no", "open", "switch", "tap", "voldown", "volup"]

DataSet, Label = load_data("LatestTraining/")
print(DataSet.shape)
print(Label.shape)

EncodedLabel = one_hot_encode(Label, 8)

X_train, X_test, y_train, y_test = train_test_split(DataSet, EncodedLabel, test_size=0.2, shuffle = True)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

def create_model(InputShape, nclasses):
  model = Sequential()
  model.add(Conv2D(16, 2,padding="same", activation="relu", input_shape=(800,581,1)))
  
  model.add(MaxPool2D())

  model.add(Conv2D(16, 2, padding="same", activation="relu"))
  model.add(MaxPool2D())

  model.add(Conv2D(16, 4, padding="same", activation="relu"))
  model.add(MaxPool2D())

  model.add(Conv2D(32, 4, padding="same", activation="relu"))
  model.add(MaxPool2D())

  model.add(Conv2D(32, 16, padding="same", activation="relu"))
  model.add(MaxPool2D())

  model.add(Conv2D(16, 32, padding="same", activation="relu"))
  model.add(MaxPool2D())

  #model.add(Conv2D(16, 64, padding="same", activation="relu"))
  #model.add(MaxPool2D())
  #model.add(Dropout(0.2))

  model.add(Flatten())
  model.add(Dense(512,activation="relu"))
  model.add(Dense(128,activation="relu")) 
  model.add(Dense(32,activation="relu"))

  model.add(Dense(nclasses, activation="softmax"))
  
  model.summary()
  return model

model = create_model((800, 581, 1), 8)

model_save_path = 'Model/'
opt = Adam(lr=0.000001)
model.compile(optimizer = opt , loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])
checkpoint = keras.callbacks.ModelCheckpoint(model_save_path+'/checkpoint_{epoch:02d}', save_freq=50)

history = model.fit(X_train,y_train, epochs = 100, validation_data = (X_test, y_test), callbacks=[checkpoint])

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(100)

plt.figure(figsize=(15, 15))
plt.subplot(2, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(2, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

fff





