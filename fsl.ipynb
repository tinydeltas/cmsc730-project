{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Import Keras and other Deep Learning dependencies\n",
    "from keras.models import Sequential\n",
    "import time\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.optimizers import *\n",
    "from keras.utils.layer_utils import get_source_inputs\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "K.set_image_data_format('channels_last')\n",
    "import os\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "# from fr_utils import *\n",
    "# from inception_blocks_v2 import *\n",
    "import numpy.random as rng\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "\n",
    "# np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/\"\n",
    "train_folder = \"training\"\n",
    "val_folder = \"test\"\n",
    "default_class = \"gesture\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer weights with mean as 0.0 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.0, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinydeltas/projects/cmsc730-project/venv/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intialize bias with mean 0.0 and standard deviation of 10^-2\n",
    "weights = initialize_weights((1000,1))\n",
    "sns.distplot(weights)\n",
    "plt.title(\"Plot of weights initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_bias(shape, dtype=None):\n",
    "    \"\"\"\n",
    "        The paper, http://www.cs.utoronto.ca/~gkoch/files/msc-thesis.pdf\n",
    "        suggests to initialize CNN layer bias with mean as 0.5 and standard deviation of 0.01\n",
    "    \"\"\"\n",
    "    return np.random.normal(loc = 0.5, scale = 1e-2, size = shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinydeltas/projects/cmsc730-project/venv/lib/python3.7/site-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01')"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Intialize bias with mean 0.5 and standard deviation of 10^-2\n",
    "bias = initialize_bias((1000,1))\n",
    "sns.distplot(bias)\n",
    "plt.title(\"Plot of biases initialized, with mean of 0.0 and standard deviation of 0.01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "def get_model(in_shape): \n",
    "    left = Input(in_shape)\n",
    "    right = Input(in_shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, \n",
    "        (10,10), \n",
    "    activation='relu', \n",
    "    input_shape=in_shape,\n",
    "    kernel_initializer=initialize_weights, \n",
    "    kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer=initialize_weights, bias_initializer=initialize_bias, kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid', kernel_regularizer=l2(1e-3), kernel_initializer=initialize_weights,bias_initializer=initialize_bias))\n",
    "    encoded_l = model(left)\n",
    "    encoded_r = model(right)\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer=initialize_bias)(L1_distance)\n",
    "    net = Model(inputs=[left, right], outputs=prediction)\n",
    "    return net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_26\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_59 (InputLayer)           [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_60 (InputLayer)           [(None, 100, 100, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_29 (Sequential)      (None, 4096)         27426112    input_59[0][0]                   \n",
      "                                                                 input_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 4096)         0           sequential_29[0][0]              \n",
      "                                                                 sequential_29[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_53 (Dense)                (None, 1)            4097        lambda_26[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 27,430,209\n",
      "Trainable params: 27,430,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# update for acoustic data\n",
    "model = get_model((100, 100, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tinydeltas/projects/cmsc730-project/venv/lib/python3.7/site-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "source": [
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from data//images/processed/train.pickle\n",
      "loading data from data//images/processed/val.pickle\n"
     ]
    }
   ],
   "source": [
    "class Loader: \n",
    "    def __init__(self, path, data_subsets = [\"train\", \"val\"]):\n",
    "        self.data = {}\n",
    "        self.categories = {}\n",
    "        self.info = {}\n",
    "        \n",
    "        for name in data_subsets:\n",
    "            file_path = os.path.join(path+\"/images/processed\", name + \".pickle\")\n",
    "            print(\"loading data from {}\".format(file_path))\n",
    "            with open(file_path,\"rb\") as f:\n",
    "                (X,c) = pickle.load(f)\n",
    "                self.data[name] = X\n",
    "                self.categories[name] = c\n",
    "\n",
    "    def get_batch(self,batch_size,s=\"train\"):\n",
    "        \"\"\"Create batch of n pairs, half same class, half different class\"\"\"\n",
    "        X=self.data[s]\n",
    "        \n",
    "        # (8, 8, 100, 100, 3)\n",
    "        n_classes, n_examples, w, h, _ = X.shape\n",
    "\n",
    "        #randomly sample several classes to use in the batch\n",
    "        categories = rng.choice(n_classes,size=(batch_size,),replace=False)\n",
    "        #initialize 2 empty arrays for the input image batch\n",
    "        pairs=[np.zeros((batch_size, h, w, 3)) for i in range(2)]\n",
    "        #initialize vector for the targets, and make one half of it '1's, so 2nd half of batch has same class\n",
    "        targets=np.zeros((batch_size,))\n",
    "        targets[batch_size//2:] = 1\n",
    "        for i in range(batch_size):\n",
    "            category = categories[i]\n",
    "            \n",
    "            idx_1 = rng.randint(0, n_examples)\n",
    "            \n",
    "            pairs[0][i,:,:,:] = X[category, idx_1].reshape(w, h, 3)\n",
    "            idx_2 = rng.randint(0, n_examples)\n",
    "            #pick images of same class for 1st half, different for 2nd\n",
    "            if i >= batch_size // 2:\n",
    "                category_2 = category  \n",
    "            else: \n",
    "                #add a random number to the category modulo n classes to ensure 2nd image has\n",
    "                # ..different category\n",
    "                category_2 = (category + rng.randint(1,n_classes)) % n_classes\n",
    "            pairs[1][i,:,:,:] = X[category_2,idx_2].reshape(w, h, 3)\n",
    "        return pairs, targets\n",
    "    \n",
    "    def generate(self, batch_size, s=\"train\"):\n",
    "        \"\"\"a generator for batches, so model.fit_generator can be used. \"\"\"\n",
    "        while True:\n",
    "            pairs, targets = self.get_batch(batch_size,s)\n",
    "            yield (pairs, targets)    \n",
    "\n",
    "    def make_oneshot_task(self,N,s=\"val\",language=None):\n",
    "        \"\"\"\n",
    "        Create pairs of test image, support set for testing N way one-shot learning. \n",
    "        \"\"\"\n",
    "        X=self.data[s]\n",
    "        n_classes, n_examples, w, h, _ = X.shape\n",
    "        indices = rng.randint(0,n_examples,size=(N,))\n",
    "        if language is not None:\n",
    "            low, high = self.categories[s][language]\n",
    "            if N > high - low:\n",
    "                raise ValueError(\"This language ({}) has less than {} letters\".format(language, N))\n",
    "            categories = rng.choice(range(low,high),size=(N,),replace=False)\n",
    "            \n",
    "        else:#if no language specified just pick a bunch of random letters\n",
    "            categories = rng.choice(range(n_classes),size=(N,),replace=False)            \n",
    "        true_category = categories[0]\n",
    "        ex1, ex2 = rng.choice(n_examples,replace=False,size=(2,))\n",
    "        test_image = np.asarray([X[true_category,ex1,:,:]]*N).reshape(N, w, h, 3)\n",
    "        support_set = X[categories,indices,:,:]\n",
    "        support_set[0,:,:] = X[true_category,ex2]\n",
    "        support_set = support_set.reshape(N, w, h, 3)\n",
    "        targets = np.zeros((N,))\n",
    "        targets[0] = 1\n",
    "        targets, test_image, support_set = shuffle(targets, test_image, support_set)\n",
    "        pairs = [test_image,support_set]\n",
    "\n",
    "        return pairs, targets\n",
    "    \n",
    "    def test_oneshot(self,model,N,k,s=\"val\",verbose=0):\n",
    "        \"\"\"Test average N way oneshot learning accuracy of a siamese neural net over k one-shot tasks\"\"\"\n",
    "        n_correct = 0\n",
    "        if verbose:\n",
    "            print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "        for i in range(k):\n",
    "            inputs, targets = self.make_oneshot_task(N,s)\n",
    "            probs = model.predict(inputs)\n",
    "            if np.argmax(probs) == np.argmax(targets):\n",
    "                n_correct+=1\n",
    "        percent_correct = (100.0*n_correct / k)\n",
    "        if verbose:\n",
    "            print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "        return percent_correct\n",
    "    \n",
    "    def train(self, model, epochs, verbosity):\n",
    "        model.fit_generator(self.generate(batch_size))\n",
    "\n",
    "loader = Loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 8, 100, 100, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[[  0,  33, 181],\n",
       "          [  0,  35, 183],\n",
       "          [ 21,  29, 177],\n",
       "          ...,\n",
       "          [134,  17, 163],\n",
       "          [ 53,  25, 174],\n",
       "          [ 71,  24, 171]],\n",
       "\n",
       "         [[ 15,  30, 178],\n",
       "          [  0,  36, 184],\n",
       "          [  0,  37, 185],\n",
       "          ...,\n",
       "          [  0,  39, 186],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  38, 186]],\n",
       "\n",
       "         [[ 89,  22, 169],\n",
       "          [  0,  37, 185],\n",
       "          [  0,  38, 186],\n",
       "          ...,\n",
       "          [  0,  45, 191],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 163, 236],\n",
       "          [  0, 157, 235],\n",
       "          [  0, 152, 234],\n",
       "          ...,\n",
       "          [  0, 182, 241],\n",
       "          [  0, 179, 240],\n",
       "          [  0, 175, 239]],\n",
       "\n",
       "         [[  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          ...,\n",
       "          [  0, 204, 245],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 197, 244]],\n",
       "\n",
       "         [[  0, 175, 239],\n",
       "          [  0, 177, 240],\n",
       "          [  0, 179, 240],\n",
       "          ...,\n",
       "          [  0, 182, 241],\n",
       "          [  0, 184, 241],\n",
       "          [  0, 182, 241]]],\n",
       "\n",
       "\n",
       "        [[[  0,  59, 200],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  36, 184],\n",
       "          ...,\n",
       "          [ 21,  29, 177],\n",
       "          [  0,  39, 186],\n",
       "          [  0,  36, 184]],\n",
       "\n",
       "         [[  0,  72, 206],\n",
       "          [  0,  70, 205],\n",
       "          [  0,  62, 201],\n",
       "          ...,\n",
       "          [  0,  47, 192],\n",
       "          [  0,  62, 201],\n",
       "          [  0,  51, 195]],\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  51, 195]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 207, 246],\n",
       "          [  0, 204, 245],\n",
       "          [  0, 202, 245],\n",
       "          ...,\n",
       "          [  0, 192, 243],\n",
       "          [  0, 189, 242],\n",
       "          [  0, 189, 242]],\n",
       "\n",
       "         [[  0, 223, 249],\n",
       "          [  0, 223, 249],\n",
       "          [  0, 223, 249],\n",
       "          ...,\n",
       "          [  0, 215, 247],\n",
       "          [  0, 215, 247],\n",
       "          [  0, 215, 247]],\n",
       "\n",
       "         [[  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          ...,\n",
       "          [  0, 215, 247],\n",
       "          [  0, 215, 247],\n",
       "          [  0, 217, 248]]],\n",
       "\n",
       "\n",
       "        [[[  0,  43, 189],\n",
       "          [ 59,  25, 173],\n",
       "          [  0,  42, 188],\n",
       "          ...,\n",
       "          [  0,  42, 188],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  40, 187]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  40, 187],\n",
       "          [  0,  42, 188],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  46, 192],\n",
       "          [  0,  43, 189]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  42, 188],\n",
       "          [  0,  37, 184],\n",
       "          ...,\n",
       "          [  0,  51, 195],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 187, 242],\n",
       "          [  0, 182, 241],\n",
       "          [  0, 182, 241],\n",
       "          ...,\n",
       "          [  0, 197, 244],\n",
       "          [  0, 194, 243],\n",
       "          [  0, 189, 242]],\n",
       "\n",
       "         [[  0, 207, 246],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 199, 244],\n",
       "          ...,\n",
       "          [  0, 204, 245],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 194, 243]],\n",
       "\n",
       "         [[  0, 212, 247],\n",
       "          [  0, 207, 246],\n",
       "          [  0, 207, 246],\n",
       "          ...,\n",
       "          [  0, 148, 233],\n",
       "          [  0, 150, 233],\n",
       "          [  0, 150, 233]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0,  78, 209],\n",
       "          [  0,  72, 206],\n",
       "          [  0,  75, 208],\n",
       "          ...,\n",
       "          [  0,  65, 203],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         [[  0,  76, 208],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  72, 206],\n",
       "          ...,\n",
       "          [  0,  76, 208],\n",
       "          [  0,  72, 206],\n",
       "          [  0,  74, 207]],\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  69, 205],\n",
       "          ...,\n",
       "          [  0,  74, 207],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  65, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 207, 246],\n",
       "          [  0, 207, 246],\n",
       "          [  0, 204, 245],\n",
       "          ...,\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 204, 245]],\n",
       "\n",
       "         [[  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          ...,\n",
       "          [  0, 226, 249],\n",
       "          [  0, 226, 249],\n",
       "          [  0, 220, 248]],\n",
       "\n",
       "         [[  0, 152, 234],\n",
       "          [  0, 152, 234],\n",
       "          [  0, 152, 234],\n",
       "          ...,\n",
       "          [  0, 234, 251],\n",
       "          [  0, 234, 251],\n",
       "          [  0, 234, 251]]],\n",
       "\n",
       "\n",
       "        [[[  0,  37, 185],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  34, 182],\n",
       "          ...,\n",
       "          [  0,  50, 194],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  48, 193],\n",
       "          [  0,  56, 198],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         [[  0,  50, 194],\n",
       "          [  0,  54, 196],\n",
       "          [  0,  43, 189],\n",
       "          ...,\n",
       "          [  0,  45, 191],\n",
       "          [  0,  57, 198],\n",
       "          [  0,  43, 189]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 194, 243],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          ...,\n",
       "          [  0, 187, 242],\n",
       "          [  0, 187, 242],\n",
       "          [  0, 184, 241]],\n",
       "\n",
       "         [[  0, 199, 244],\n",
       "          [  0, 199, 244],\n",
       "          [  0, 199, 244],\n",
       "          ...,\n",
       "          [  0, 209, 246],\n",
       "          [  0, 207, 246],\n",
       "          [  0, 204, 245]],\n",
       "\n",
       "         [[  0, 146, 232],\n",
       "          [  0, 161, 236],\n",
       "          [  0, 170, 238],\n",
       "          ...,\n",
       "          [  0, 231, 250],\n",
       "          [  0, 231, 250],\n",
       "          [  0, 228, 250]]],\n",
       "\n",
       "\n",
       "        [[[  0,  79, 210],\n",
       "          [  0,  78, 209],\n",
       "          [  0,  75, 208],\n",
       "          ...,\n",
       "          [  0,  48, 193],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         [[  0,  67, 204],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  76, 208],\n",
       "          ...,\n",
       "          [  0,  62, 201],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  67, 204]],\n",
       "\n",
       "         [[  0,  59, 200],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  74, 207],\n",
       "          ...,\n",
       "          [  0,  75, 208],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  64, 202]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 199, 244],\n",
       "          [  0, 197, 244],\n",
       "          [  0, 197, 244],\n",
       "          ...,\n",
       "          [  0, 197, 244],\n",
       "          [  0, 197, 244],\n",
       "          [  0, 194, 243]],\n",
       "\n",
       "         [[  0, 217, 248],\n",
       "          [  0, 215, 247],\n",
       "          [  0, 215, 247],\n",
       "          ...,\n",
       "          [  0, 197, 244],\n",
       "          [  0, 194, 243],\n",
       "          [  0, 192, 243]],\n",
       "\n",
       "         [[  0, 234, 251],\n",
       "          [  0, 234, 251],\n",
       "          [  0, 234, 251],\n",
       "          ...,\n",
       "          [  0, 194, 243],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 189, 242]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[  0,  33, 182],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  37, 184],\n",
       "          ...,\n",
       "          [ 21,  29, 177],\n",
       "          [  9,  30, 179],\n",
       "          [ 89,  22, 169]],\n",
       "\n",
       "         [[  0,  49, 194],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  42, 188],\n",
       "          ...,\n",
       "          [  0,  42, 188],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         [[  0,  52, 196],\n",
       "          [  0,  57, 198],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  49, 194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 207, 246],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 204, 245],\n",
       "          ...,\n",
       "          [  0, 204, 245],\n",
       "          [  0, 204, 245],\n",
       "          [  0, 197, 244]],\n",
       "\n",
       "         [[  0, 148, 233],\n",
       "          [  0, 144, 231],\n",
       "          [  0, 144, 231],\n",
       "          ...,\n",
       "          [  0, 168, 237],\n",
       "          [  0, 170, 238],\n",
       "          [  0, 165, 237]],\n",
       "\n",
       "         [[  0,  71, 206],\n",
       "          [  0,  82, 211],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  85, 212],\n",
       "          [  0,  92, 215],\n",
       "          [  0,  85, 212]]],\n",
       "\n",
       "\n",
       "        [[[  0,  41, 188],\n",
       "          [  0,  33, 181],\n",
       "          [  0,  46, 192],\n",
       "          ...,\n",
       "          [  0,  59, 200],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  50, 194]],\n",
       "\n",
       "         [[  0,  52, 196],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  61, 201],\n",
       "          ...,\n",
       "          [  0,  65, 203],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  60, 200]],\n",
       "\n",
       "         [[  0,  57, 198],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  62, 201],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 204, 245],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 202, 245],\n",
       "          ...,\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 189, 242]],\n",
       "\n",
       "         [[  0, 172, 238],\n",
       "          [  0, 172, 238],\n",
       "          [  0, 172, 238],\n",
       "          ...,\n",
       "          [  0, 163, 236],\n",
       "          [  0, 163, 236],\n",
       "          [  0, 161, 236]],\n",
       "\n",
       "         [[  0, 110, 221],\n",
       "          [  0, 124, 226],\n",
       "          [  0, 124, 226],\n",
       "          ...,\n",
       "          [  0,  88, 213],\n",
       "          [  0,  85, 212],\n",
       "          [  0,  72, 206]]],\n",
       "\n",
       "\n",
       "        [[[  0,  42, 188],\n",
       "          [  0,  37, 185],\n",
       "          [  0,  47, 192],\n",
       "          ...,\n",
       "          [  0,  36, 184],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  36, 184]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  45, 191],\n",
       "          [  0,  56, 198],\n",
       "          ...,\n",
       "          [  0,  49, 194],\n",
       "          [  0,  50, 194],\n",
       "          [  0,  42, 188]],\n",
       "\n",
       "         [[  0,  46, 192],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  58, 199],\n",
       "          ...,\n",
       "          [  0,  54, 196],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 197, 244],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          ...,\n",
       "          [  0, 202, 245],\n",
       "          [  0, 202, 245],\n",
       "          [  0, 197, 244]],\n",
       "\n",
       "         [[  0, 142, 231],\n",
       "          [  0, 142, 231],\n",
       "          [  0, 142, 231],\n",
       "          ...,\n",
       "          [  0, 146, 232],\n",
       "          [  0, 146, 232],\n",
       "          [  0, 146, 232]],\n",
       "\n",
       "         [[  0,  72, 206],\n",
       "          [  0,  83, 211],\n",
       "          [  0,  86, 213],\n",
       "          ...,\n",
       "          [  0,  72, 206],\n",
       "          [  0,  82, 211],\n",
       "          [  0,  76, 208]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 28,  28, 177],\n",
       "          [ 15,  30, 178],\n",
       "          [  0,  36, 184],\n",
       "          ...,\n",
       "          [  0,  32, 180],\n",
       "          [ 34,  27, 176],\n",
       "          [  0,  32, 180]],\n",
       "\n",
       "         [[  0,  41, 188],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  39, 186],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         [[  0,  40, 187],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  45, 191],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 182, 241],\n",
       "          [  0, 177, 240],\n",
       "          [  0, 177, 240],\n",
       "          ...,\n",
       "          [  0, 194, 243],\n",
       "          [  0, 194, 243],\n",
       "          [  0, 189, 242]],\n",
       "\n",
       "         [[  0, 107, 220],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 115, 223],\n",
       "          ...,\n",
       "          [  0, 140, 230],\n",
       "          [  0, 138, 230],\n",
       "          [  0, 130, 228]],\n",
       "\n",
       "         [[  0,  78, 209],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  69, 205],\n",
       "          ...,\n",
       "          [  0,  71, 206],\n",
       "          [  0,  79, 210],\n",
       "          [  0,  74, 207]]],\n",
       "\n",
       "\n",
       "        [[[  0,  59, 200],\n",
       "          [  0,  72, 206],\n",
       "          [  0,  65, 203],\n",
       "          ...,\n",
       "          [  0,  48, 193],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  42, 188]],\n",
       "\n",
       "         [[  0,  76, 208],\n",
       "          [  0,  76, 208],\n",
       "          [  0,  82, 211],\n",
       "          ...,\n",
       "          [  0,  62, 201],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  69, 205],\n",
       "          [  0,  76, 208],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 215, 247],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          ...,\n",
       "          [  0, 212, 247],\n",
       "          [  0, 212, 247],\n",
       "          [  0, 207, 246]],\n",
       "\n",
       "         [[  0, 182, 241],\n",
       "          [  0, 179, 240],\n",
       "          [  0, 179, 240],\n",
       "          ...,\n",
       "          [  0, 197, 244],\n",
       "          [  0, 194, 243],\n",
       "          [  0, 192, 243]],\n",
       "\n",
       "         [[  0, 103, 219],\n",
       "          [  0, 102, 218],\n",
       "          [  0, 115, 223],\n",
       "          ...,\n",
       "          [  0, 126, 226],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 114, 222]]],\n",
       "\n",
       "\n",
       "        [[[  0,  58, 199],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  75, 208],\n",
       "          ...,\n",
       "          [  0,  67, 204],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  50, 194]],\n",
       "\n",
       "         [[  0,  72, 206],\n",
       "          [  0,  79, 210],\n",
       "          [  0,  76, 208],\n",
       "          ...,\n",
       "          [  0,  65, 203],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  64, 202]],\n",
       "\n",
       "         [[  0,  59, 200],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  66, 203],\n",
       "          ...,\n",
       "          [  0,  71, 206],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  69, 205]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 209, 246],\n",
       "          [  0, 204, 245],\n",
       "          [  0, 204, 245],\n",
       "          ...,\n",
       "          [  0, 209, 246],\n",
       "          [  0, 207, 246],\n",
       "          [  0, 202, 245]],\n",
       "\n",
       "         [[  0, 175, 239],\n",
       "          [  0, 172, 238],\n",
       "          [  0, 172, 238],\n",
       "          ...,\n",
       "          [  0, 175, 239],\n",
       "          [  0, 175, 239],\n",
       "          [  0, 172, 238]],\n",
       "\n",
       "         [[  0,  75, 208],\n",
       "          [  0,  95, 216],\n",
       "          [  0,  88, 213],\n",
       "          ...,\n",
       "          [  0, 108, 221],\n",
       "          [  0, 103, 219],\n",
       "          [  0, 102, 218]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[  0,  37, 185],\n",
       "          [  0,  52, 196],\n",
       "          [  0,  55, 197],\n",
       "          ...,\n",
       "          [  0,  58, 199],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  43, 189]],\n",
       "\n",
       "         [[  0,  51, 195],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  62, 201],\n",
       "          ...,\n",
       "          [  0,  69, 205],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         [[  0,  55, 197],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  59, 200],\n",
       "          ...,\n",
       "          [  0,  60, 200],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 223, 249],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 217, 248],\n",
       "          ...,\n",
       "          [  0, 217, 248],\n",
       "          [  0, 217, 248],\n",
       "          [  0, 217, 248]],\n",
       "\n",
       "         [[  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          ...,\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243],\n",
       "          [  0, 192, 243]],\n",
       "\n",
       "         [[  0, 114, 222],\n",
       "          [  0, 117, 224],\n",
       "          [  0, 115, 223],\n",
       "          ...,\n",
       "          [  0,  57, 198],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  46, 192]]],\n",
       "\n",
       "\n",
       "        [[[  9,  30, 179],\n",
       "          [  0,  40, 187],\n",
       "          [  0,  33, 181],\n",
       "          ...,\n",
       "          [ 53,  25, 174],\n",
       "          [  0,  42, 188],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         [[  0,  59, 200],\n",
       "          [  0,  57, 198],\n",
       "          [  0,  52, 196],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  48, 193]],\n",
       "\n",
       "         [[  0,  61, 201],\n",
       "          [  0,  62, 201],\n",
       "          [  0,  61, 201],\n",
       "          ...,\n",
       "          [  0,  62, 201],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  58, 199]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 223, 249],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          ...,\n",
       "          [  0, 217, 248],\n",
       "          [  0, 217, 248],\n",
       "          [  0, 217, 248]],\n",
       "\n",
       "         [[  0, 187, 242],\n",
       "          [  0, 187, 242],\n",
       "          [  0, 187, 242],\n",
       "          ...,\n",
       "          [  0, 189, 242],\n",
       "          [  0, 189, 242],\n",
       "          [  0, 187, 242]],\n",
       "\n",
       "         [[  0,  78, 209],\n",
       "          [  0,  82, 211],\n",
       "          [  0,  95, 216],\n",
       "          ...,\n",
       "          [  0, 123, 225],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 114, 222]]],\n",
       "\n",
       "\n",
       "        [[[  0,  51, 195],\n",
       "          [  0,  52, 196],\n",
       "          [  0,  51, 195],\n",
       "          ...,\n",
       "          [  0,  54, 196],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  48, 193]],\n",
       "\n",
       "         [[  0,  60, 200],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  72, 206],\n",
       "          ...,\n",
       "          [  0,  72, 206],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  57, 198]],\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  70, 205],\n",
       "          ...,\n",
       "          [  0,  75, 208],\n",
       "          [  0,  70, 205],\n",
       "          [  0,  74, 207]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 220, 248],\n",
       "          [  0, 217, 248],\n",
       "          [  0, 215, 247],\n",
       "          ...,\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 223, 249]],\n",
       "\n",
       "         [[  0, 177, 240],\n",
       "          [  0, 175, 239],\n",
       "          [  0, 175, 239],\n",
       "          ...,\n",
       "          [  0, 177, 240],\n",
       "          [  0, 177, 240],\n",
       "          [  0, 177, 240]],\n",
       "\n",
       "         [[  0,  98, 217],\n",
       "          [  0,  86, 213],\n",
       "          [  0, 114, 222],\n",
       "          ...,\n",
       "          [  0,  89, 214],\n",
       "          [  0,  92, 215],\n",
       "          [  0,  94, 216]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0,  32, 180],\n",
       "          [  0,  34, 182],\n",
       "          [  0,  41, 188],\n",
       "          ...,\n",
       "          [  0,  40, 187],\n",
       "          [ 40,  27, 175],\n",
       "          [ 15,  30, 178]],\n",
       "\n",
       "         [[  0,  48, 193],\n",
       "          [  0,  50, 194],\n",
       "          [  0,  51, 195],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  47, 192]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  51, 195],\n",
       "          ...,\n",
       "          [  0,  42, 188],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  38, 186]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 220, 248],\n",
       "          [  0, 217, 248],\n",
       "          [  0, 217, 248],\n",
       "          ...,\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 217, 248]],\n",
       "\n",
       "         [[  0, 152, 234],\n",
       "          [  0, 150, 233],\n",
       "          [  0, 150, 233],\n",
       "          ...,\n",
       "          [  0, 168, 237],\n",
       "          [  0, 165, 237],\n",
       "          [  0, 163, 236]],\n",
       "\n",
       "         [[  0,  69, 205],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  70, 205],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  81, 210]]],\n",
       "\n",
       "\n",
       "        [[[  0,  36, 184],\n",
       "          [  3,  31, 179],\n",
       "          [  0,  33, 181],\n",
       "          ...,\n",
       "          [  0,  39, 186],\n",
       "          [  0,  39, 186],\n",
       "          [  0,  41, 188]],\n",
       "\n",
       "         [[  0,  48, 193],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  52, 196],\n",
       "          ...,\n",
       "          [  0,  55, 197],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  49, 194]],\n",
       "\n",
       "         [[  0,  49, 194],\n",
       "          [  0,  37, 184],\n",
       "          [  0,  48, 193],\n",
       "          ...,\n",
       "          [  0,  45, 191],\n",
       "          [  0,  46, 192],\n",
       "          [  0,  47, 192]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 212, 247],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          ...,\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 217, 248]],\n",
       "\n",
       "         [[  0, 148, 233],\n",
       "          [  0, 148, 233],\n",
       "          [  0, 150, 233],\n",
       "          ...,\n",
       "          [  0, 152, 234],\n",
       "          [  0, 152, 234],\n",
       "          [  0, 150, 233]],\n",
       "\n",
       "         [[  0,  94, 216],\n",
       "          [  0,  95, 216],\n",
       "          [  0,  97, 217],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  60, 200]]],\n",
       "\n",
       "\n",
       "        [[[ 40,  27, 175],\n",
       "          [ 46,  26, 174],\n",
       "          [  0,  37, 185],\n",
       "          ...,\n",
       "          [  0,  36, 184],\n",
       "          [  0,  40, 187],\n",
       "          [  0,  33, 181]],\n",
       "\n",
       "         [[  0,  37, 184],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [  0,  47, 192],\n",
       "          [  0,  50, 194],\n",
       "          [  0,  40, 187]],\n",
       "\n",
       "         [[  0,  42, 188],\n",
       "          [  0,  45, 191],\n",
       "          [  0,  48, 193],\n",
       "          ...,\n",
       "          [  0,  43, 189],\n",
       "          [  0,  38, 186],\n",
       "          [  0,  41, 188]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 223, 249],\n",
       "          [  0, 220, 248],\n",
       "          [  0, 220, 248],\n",
       "          ...,\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246],\n",
       "          [  0, 209, 246]],\n",
       "\n",
       "         [[  0, 152, 234],\n",
       "          [  0, 154, 234],\n",
       "          [  0, 157, 235],\n",
       "          ...,\n",
       "          [  0, 150, 233],\n",
       "          [  0, 150, 233],\n",
       "          [  0, 148, 233]],\n",
       "\n",
       "         [[  0,  89, 214],\n",
       "          [  0,  94, 216],\n",
       "          [  0,  95, 216],\n",
       "          ...,\n",
       "          [  0,  97, 217],\n",
       "          [  0,  92, 215],\n",
       "          [  0,  82, 211]]]],\n",
       "\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "\n",
       "       [[[[  0,  57, 198],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  52, 196],\n",
       "          ...,\n",
       "          [  0,  50, 194],\n",
       "          [  0,  46, 192],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         [[  0,  67, 204],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  61, 201],\n",
       "          ...,\n",
       "          [  0,  69, 205],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  64, 202]],\n",
       "\n",
       "         [[  0,  64, 202],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  71, 206],\n",
       "          ...,\n",
       "          [  0,  58, 199],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  75, 208],\n",
       "          [  0,  76, 208],\n",
       "          [  0,  72, 206],\n",
       "          ...,\n",
       "          [  0, 103, 219],\n",
       "          [  0,  95, 216],\n",
       "          [  0,  85, 212]],\n",
       "\n",
       "         [[  0,  79, 210],\n",
       "          [  0,  78, 209],\n",
       "          [  0,  81, 210],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  98, 217],\n",
       "          [  0,  92, 215]],\n",
       "\n",
       "         [[  0,  76, 208],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  71, 206],\n",
       "          ...,\n",
       "          [  0,  75, 208],\n",
       "          [  0, 103, 219],\n",
       "          [  0, 100, 218]]],\n",
       "\n",
       "\n",
       "        [[[  0,  75, 208],\n",
       "          [  0,  79, 210],\n",
       "          [  0,  70, 205],\n",
       "          ...,\n",
       "          [  0,  85, 212],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  79, 210]],\n",
       "\n",
       "         [[  0,  89, 214],\n",
       "          [  0,  92, 215],\n",
       "          [  0,  92, 215],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  86, 213],\n",
       "          [  0,  95, 216]],\n",
       "\n",
       "         [[  0,  92, 215],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  89, 214],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  88, 213],\n",
       "          [  0,  78, 209]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 108, 221],\n",
       "          [  0, 114, 222],\n",
       "          [  0, 105, 220],\n",
       "          ...,\n",
       "          [  0,  91, 214],\n",
       "          [  0,  88, 213],\n",
       "          [  0,  97, 217]],\n",
       "\n",
       "         [[  0, 119, 224],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 115, 223],\n",
       "          ...,\n",
       "          [  0,  88, 213],\n",
       "          [  0,  86, 213],\n",
       "          [  0, 100, 218]],\n",
       "\n",
       "         [[  0, 114, 222],\n",
       "          [  0,  98, 217],\n",
       "          [  0, 110, 221],\n",
       "          ...,\n",
       "          [  0,  88, 213],\n",
       "          [  0,  89, 214],\n",
       "          [  0,  92, 215]]],\n",
       "\n",
       "\n",
       "        [[[  9,  30, 179],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  34, 182],\n",
       "          ...,\n",
       "          [100,  21, 168],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  40, 187]],\n",
       "\n",
       "         [[  0,  47, 192],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  45, 191],\n",
       "          ...,\n",
       "          [  0,  58, 199],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         [[  0,  46, 192],\n",
       "          [  0,  56, 198],\n",
       "          [  0,  48, 193],\n",
       "          ...,\n",
       "          [  0,  55, 197],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  65, 203],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  82, 211],\n",
       "          [  0,  89, 214],\n",
       "          [  0,  91, 214]],\n",
       "\n",
       "         [[  0,  67, 204],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  71, 206],\n",
       "          ...,\n",
       "          [  0, 103, 219],\n",
       "          [  0, 105, 220],\n",
       "          [  0,  92, 215]],\n",
       "\n",
       "         [[  0,  51, 195],\n",
       "          [  0,  54, 196],\n",
       "          [  0,  58, 199],\n",
       "          ...,\n",
       "          [  0,  95, 216],\n",
       "          [  0, 100, 218],\n",
       "          [  0,  38, 186]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0,  40, 187],\n",
       "          [  0,  37, 184],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [ 34,  27, 176],\n",
       "          [  0,  43, 189],\n",
       "          [  0,  37, 185]],\n",
       "\n",
       "         [[  0,  47, 192],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  46, 192],\n",
       "          ...,\n",
       "          [  0,  41, 188],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  55, 197]],\n",
       "\n",
       "         [[  0,  47, 192],\n",
       "          [  0,  50, 194],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [  0,  47, 192],\n",
       "          [  0,  48, 193],\n",
       "          [  0,  40, 187]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  69, 205],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  78, 209],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  62, 201]],\n",
       "\n",
       "         [[  0,  76, 208],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  66, 203],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  57, 198]],\n",
       "\n",
       "         [[  0,  51, 195],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  40, 187],\n",
       "          ...,\n",
       "          [  0,  49, 194],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  51, 195]]],\n",
       "\n",
       "\n",
       "        [[[  9,  30, 179],\n",
       "          [ 71,  24, 171],\n",
       "          [  0,  37, 184],\n",
       "          ...,\n",
       "          [128,  18, 164],\n",
       "          [ 59,  25, 173],\n",
       "          [  0,  37, 185]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [  0,  34, 182],\n",
       "          [  0,  43, 189],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         [[  0,  44, 190],\n",
       "          [  0,  49, 194],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [ 28,  28, 177],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  47, 192]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  72, 206],\n",
       "          [  0,  83, 211],\n",
       "          ...,\n",
       "          [  0,  55, 197],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  60, 200]],\n",
       "\n",
       "         [[  0,  70, 205],\n",
       "          [  0,  85, 212],\n",
       "          [  0,  92, 215],\n",
       "          ...,\n",
       "          [  0,  50, 194],\n",
       "          [  0,  62, 201],\n",
       "          [  0,  65, 203]],\n",
       "\n",
       "         [[  0,  69, 205],\n",
       "          [  0,  62, 201],\n",
       "          [  0,  65, 203],\n",
       "          ...,\n",
       "          [  0,  56, 198],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  67, 204]]],\n",
       "\n",
       "\n",
       "        [[[  0,  37, 185],\n",
       "          [ 28,  28, 177],\n",
       "          [ 77,  23, 171],\n",
       "          ...,\n",
       "          [ 89,  22, 169],\n",
       "          [  0,  37, 185],\n",
       "          [  0,  33, 181]],\n",
       "\n",
       "         [[  0,  51, 195],\n",
       "          [  0,  37, 185],\n",
       "          [  0,  33, 182],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  48, 193],\n",
       "          [  0,  56, 198]],\n",
       "\n",
       "         [[  0,  39, 186],\n",
       "          [  0,  34, 182],\n",
       "          [  0,  41, 188],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  56, 198],\n",
       "          [  0,  38, 186]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  61, 201],\n",
       "          [  0,  64, 202],\n",
       "          [  0,  60, 200],\n",
       "          ...,\n",
       "          [  0,  59, 200],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  69, 205]],\n",
       "\n",
       "         [[  0,  71, 206],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  66, 203],\n",
       "          ...,\n",
       "          [  0,  65, 203],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  56, 198]],\n",
       "\n",
       "         [[  0,  61, 201],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  64, 202],\n",
       "          ...,\n",
       "          [  0,  45, 191],\n",
       "          [  0,  33, 182],\n",
       "          [128,  18, 164]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[ 89,  22, 169],\n",
       "          [106,  20, 167],\n",
       "          [ 46,  26, 174],\n",
       "          ...,\n",
       "          [106,  20, 167],\n",
       "          [ 28,  28, 177],\n",
       "          [149,  16, 161]],\n",
       "\n",
       "         [[ 71,  24, 171],\n",
       "          [ 40,  27, 175],\n",
       "          [  0,  37, 184],\n",
       "          ...,\n",
       "          [ 71,  24, 171],\n",
       "          [ 15,  30, 178],\n",
       "          [ 40,  27, 175]],\n",
       "\n",
       "         [[ 15,  30, 178],\n",
       "          [  0,  32, 180],\n",
       "          [  0,  33, 182],\n",
       "          ...,\n",
       "          [  9,  30, 179],\n",
       "          [ 15,  30, 178],\n",
       "          [ 21,  29, 177]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0,  92, 215],\n",
       "          [  0,  78, 209],\n",
       "          [  0,  75, 208],\n",
       "          ...,\n",
       "          [  0, 108, 221],\n",
       "          [  0, 110, 221],\n",
       "          [  0, 115, 223]],\n",
       "\n",
       "         [[  0,  98, 217],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  89, 214],\n",
       "          ...,\n",
       "          [  0, 102, 218],\n",
       "          [  0,  95, 216],\n",
       "          [  0,  91, 214]],\n",
       "\n",
       "         [[  0,  79, 210],\n",
       "          [  0,  83, 211],\n",
       "          [  0,  83, 211],\n",
       "          ...,\n",
       "          [  0,  82, 211],\n",
       "          [  0,  81, 210],\n",
       "          [  0,  81, 210]]],\n",
       "\n",
       "\n",
       "        [[[  0,  44, 190],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  42, 188],\n",
       "          ...,\n",
       "          [  0,  49, 194],\n",
       "          [  0,  52, 196],\n",
       "          [  0,  47, 192]],\n",
       "\n",
       "         [[  0,  62, 201],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  56, 198],\n",
       "          ...,\n",
       "          [  0,  62, 201],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  60, 200]],\n",
       "\n",
       "         [[  0,  57, 198],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  61, 201],\n",
       "          ...,\n",
       "          [  0,  67, 204],\n",
       "          [  0,  67, 204],\n",
       "          [  0,  58, 199]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 123, 225],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 134, 229],\n",
       "          ...,\n",
       "          [  0, 107, 220],\n",
       "          [  0, 114, 222],\n",
       "          [  0, 124, 226]],\n",
       "\n",
       "         [[  0, 132, 228],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 130, 228],\n",
       "          ...,\n",
       "          [  0, 124, 226],\n",
       "          [  0, 124, 226],\n",
       "          [  0, 130, 228]],\n",
       "\n",
       "         [[  0, 105, 220],\n",
       "          [  0, 100, 218],\n",
       "          [  0, 117, 224],\n",
       "          ...,\n",
       "          [  0, 115, 223],\n",
       "          [  0, 119, 224],\n",
       "          [  0, 128, 227]]],\n",
       "\n",
       "\n",
       "        [[[ 53,  25, 174],\n",
       "          [ 28,  28, 177],\n",
       "          [ 40,  27, 175],\n",
       "          ...,\n",
       "          [  0,  33, 181],\n",
       "          [ 59,  25, 173],\n",
       "          [ 28,  28, 177]],\n",
       "\n",
       "         [[  0,  41, 188],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  35, 183],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         [[  0,  41, 188],\n",
       "          [  0,  42, 188],\n",
       "          [  0,  43, 189],\n",
       "          ...,\n",
       "          [  0,  41, 188],\n",
       "          [  0,  38, 186],\n",
       "          [  0,  41, 188]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 105, 220],\n",
       "          [  0,  94, 216],\n",
       "          [  0,  88, 213],\n",
       "          ...,\n",
       "          [  0, 110, 221],\n",
       "          [  0, 112, 222],\n",
       "          [  0,  98, 217]],\n",
       "\n",
       "         [[  0, 121, 225],\n",
       "          [  0, 119, 224],\n",
       "          [  0, 117, 224],\n",
       "          ...,\n",
       "          [  0, 117, 224],\n",
       "          [  0, 114, 222],\n",
       "          [  0, 103, 219]],\n",
       "\n",
       "         [[  0, 110, 221],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 105, 220],\n",
       "          ...,\n",
       "          [  0, 102, 218],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  91, 214]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  9,  30, 179],\n",
       "          [ 59,  25, 173],\n",
       "          [  0,  33, 181],\n",
       "          ...,\n",
       "          [ 15,  30, 178],\n",
       "          [ 95,  21, 168],\n",
       "          [ 53,  25, 174]],\n",
       "\n",
       "         [[  0,  41, 188],\n",
       "          [  0,  43, 189],\n",
       "          [  0,  44, 190],\n",
       "          ...,\n",
       "          [  0,  54, 196],\n",
       "          [  0,  36, 184],\n",
       "          [  0,  36, 184]],\n",
       "\n",
       "         [[  0,  36, 184],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  48, 193],\n",
       "          ...,\n",
       "          [  0,  49, 194],\n",
       "          [  0,  42, 188],\n",
       "          [  0,  33, 182]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 105, 220],\n",
       "          [  0, 100, 218],\n",
       "          [  0, 103, 219],\n",
       "          ...,\n",
       "          [  0, 108, 221],\n",
       "          [  0, 117, 224],\n",
       "          [  0, 107, 220]],\n",
       "\n",
       "         [[  0,  91, 214],\n",
       "          [  0,  85, 212],\n",
       "          [  0,  89, 214],\n",
       "          ...,\n",
       "          [  0, 112, 222],\n",
       "          [  0, 110, 221],\n",
       "          [  0, 117, 224]],\n",
       "\n",
       "         [[  0,  72, 206],\n",
       "          [  0,  76, 208],\n",
       "          [  0,  62, 201],\n",
       "          ...,\n",
       "          [  0,  74, 207],\n",
       "          [  0,  65, 203],\n",
       "          [  0, 100, 218]]],\n",
       "\n",
       "\n",
       "        [[[  0,  42, 188],\n",
       "          [  0,  52, 196],\n",
       "          [  0,  55, 197],\n",
       "          ...,\n",
       "          [  0,  56, 198],\n",
       "          [  0,  38, 186],\n",
       "          [  0,  54, 196]],\n",
       "\n",
       "         [[  0,  65, 203],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  58, 199],\n",
       "          ...,\n",
       "          [  0,  62, 201],\n",
       "          [  0,  54, 196],\n",
       "          [  0,  57, 198]],\n",
       "\n",
       "         [[  0,  67, 204],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  59, 200],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  54, 196],\n",
       "          [  0,  51, 195]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 150, 233],\n",
       "          [  0, 148, 233],\n",
       "          [  0, 144, 231],\n",
       "          ...,\n",
       "          [  0, 112, 222],\n",
       "          [  0, 115, 223],\n",
       "          [  0, 123, 225]],\n",
       "\n",
       "         [[  0, 132, 228],\n",
       "          [  0, 134, 229],\n",
       "          [  0, 134, 229],\n",
       "          ...,\n",
       "          [  0, 123, 225],\n",
       "          [  0, 126, 226],\n",
       "          [  0, 130, 228]],\n",
       "\n",
       "         [[  0, 121, 225],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 121, 225],\n",
       "          ...,\n",
       "          [  0, 134, 229],\n",
       "          [  0, 132, 228],\n",
       "          [  0, 134, 229]]],\n",
       "\n",
       "\n",
       "        [[[  0,  33, 182],\n",
       "          [  0,  33, 182],\n",
       "          [ 40,  27, 175],\n",
       "          ...,\n",
       "          [  3,  31, 179],\n",
       "          [  0,  41, 188],\n",
       "          [  0,  37, 185]],\n",
       "\n",
       "         [[  0,  37, 184],\n",
       "          [  0,  38, 186],\n",
       "          [  0,  45, 191],\n",
       "          ...,\n",
       "          [  0,  49, 194],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  48, 193]],\n",
       "\n",
       "         [[  0,  38, 186],\n",
       "          [  0,  43, 189],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  52, 196],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 110, 221],\n",
       "          [  0, 110, 221],\n",
       "          [  0, 117, 224],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  85, 212],\n",
       "          [  0,  89, 214]],\n",
       "\n",
       "         [[  0, 119, 224],\n",
       "          [  0, 119, 224],\n",
       "          [  0, 123, 225],\n",
       "          ...,\n",
       "          [  0,  88, 213],\n",
       "          [  0,  94, 216],\n",
       "          [  0,  85, 212]],\n",
       "\n",
       "         [[  0, 123, 225],\n",
       "          [  0, 115, 223],\n",
       "          [  0, 112, 222],\n",
       "          ...,\n",
       "          [  0,  76, 208],\n",
       "          [  0,  79, 210],\n",
       "          [  0,  79, 210]]]],\n",
       "\n",
       "\n",
       "\n",
       "       [[[[  0,  58, 199],\n",
       "          [  0,  62, 201],\n",
       "          [  0,  57, 198],\n",
       "          ...,\n",
       "          [  0,  58, 199],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  44, 190]],\n",
       "\n",
       "         [[  0,  58, 199],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  58, 199],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  67, 204]],\n",
       "\n",
       "         [[  0,  46, 192],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  65, 203],\n",
       "          ...,\n",
       "          [  0,  61, 201],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  66, 203]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 114, 222],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 110, 221],\n",
       "          ...,\n",
       "          [  0, 108, 221],\n",
       "          [  0, 110, 221],\n",
       "          [  0, 112, 222]],\n",
       "\n",
       "         [[  0,  97, 217],\n",
       "          [  0, 100, 218],\n",
       "          [  0, 105, 220],\n",
       "          ...,\n",
       "          [  0,  82, 211],\n",
       "          [  0,  78, 209],\n",
       "          [  0,  82, 211]],\n",
       "\n",
       "         [[  0,  75, 208],\n",
       "          [  0,  88, 213],\n",
       "          [  0,  94, 216],\n",
       "          ...,\n",
       "          [  0,  69, 205],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  65, 203]]],\n",
       "\n",
       "\n",
       "        [[[  0,  45, 191],\n",
       "          [  0,  33, 182],\n",
       "          [  0,  52, 196],\n",
       "          ...,\n",
       "          [  0,  40, 187],\n",
       "          [  0,  47, 192],\n",
       "          [  0,  37, 185]],\n",
       "\n",
       "         [[  0,  64, 202],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  62, 201],\n",
       "          ...,\n",
       "          [  0,  50, 194],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  56, 198]],\n",
       "\n",
       "         [[  0,  59, 200],\n",
       "          [  0,  55, 197],\n",
       "          [  0,  39, 186],\n",
       "          ...,\n",
       "          [  0,  60, 200],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 105, 220],\n",
       "          [  0, 103, 219],\n",
       "          [  0, 105, 220],\n",
       "          ...,\n",
       "          [  0, 105, 220],\n",
       "          [  0, 105, 220],\n",
       "          [  0, 103, 219]],\n",
       "\n",
       "         [[  0,  89, 214],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  98, 217],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  91, 214],\n",
       "          [  0,  88, 213]],\n",
       "\n",
       "         [[  0,  71, 206],\n",
       "          [  0,  60, 200],\n",
       "          [  0,  69, 205],\n",
       "          ...,\n",
       "          [  0,  86, 213],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  66, 203]]],\n",
       "\n",
       "\n",
       "        [[[106,  20, 167],\n",
       "          [  0,  33, 181],\n",
       "          [ 46,  26, 174],\n",
       "          ...,\n",
       "          [  0,  36, 184],\n",
       "          [ 65,  24, 172],\n",
       "          [  0,  38, 186]],\n",
       "\n",
       "         [[  0,  48, 193],\n",
       "          [  0,  58, 199],\n",
       "          [  0,  57, 198],\n",
       "          ...,\n",
       "          [  0,  46, 192],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  45, 191]],\n",
       "\n",
       "         [[  0,  42, 188],\n",
       "          [  0,  50, 194],\n",
       "          [  0,  47, 192],\n",
       "          ...,\n",
       "          [  0,  54, 196],\n",
       "          [  0,  51, 195],\n",
       "          [  0,  50, 194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 108, 221],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 108, 221],\n",
       "          ...,\n",
       "          [  0, 115, 223],\n",
       "          [  0, 115, 223],\n",
       "          [  0, 114, 222]],\n",
       "\n",
       "         [[  0,  92, 215],\n",
       "          [  0,  94, 216],\n",
       "          [  0,  92, 215],\n",
       "          ...,\n",
       "          [  0, 100, 218],\n",
       "          [  0, 105, 220],\n",
       "          [  0, 105, 220]],\n",
       "\n",
       "         [[  0,  75, 208],\n",
       "          [  0,  71, 206],\n",
       "          [  0,  66, 203],\n",
       "          ...,\n",
       "          [  0,  85, 212],\n",
       "          [  0,  78, 209],\n",
       "          [  0,  79, 210]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[  0,  55, 197],\n",
       "          [  0,  45, 191],\n",
       "          [  0,  58, 199],\n",
       "          ...,\n",
       "          [  0,  64, 202],\n",
       "          [  0,  54, 196],\n",
       "          [  0,  54, 196]],\n",
       "\n",
       "         [[  0,  75, 208],\n",
       "          [  0,  74, 207],\n",
       "          [  0,  82, 211],\n",
       "          ...,\n",
       "          [  0,  75, 208],\n",
       "          [  0,  79, 210],\n",
       "          [  0,  86, 213]],\n",
       "\n",
       "         [[  0,  88, 213],\n",
       "          [  0,  85, 212],\n",
       "          [  0,  81, 210],\n",
       "          ...,\n",
       "          [  0,  78, 209],\n",
       "          [  0,  81, 210],\n",
       "          [  0,  83, 211]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 126, 226],\n",
       "          [  0, 124, 226],\n",
       "          [  0, 126, 226],\n",
       "          ...,\n",
       "          [  0, 130, 228],\n",
       "          [  0, 130, 228],\n",
       "          [  0, 128, 227]],\n",
       "\n",
       "         [[  0, 123, 225],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 121, 225],\n",
       "          ...,\n",
       "          [  0, 115, 223],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 119, 224]],\n",
       "\n",
       "         [[  0, 110, 221],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 112, 222],\n",
       "          ...,\n",
       "          [  0,  92, 215],\n",
       "          [  0,  98, 217],\n",
       "          [  0,  97, 217]]],\n",
       "\n",
       "\n",
       "        [[[  0,  44, 190],\n",
       "          [  0,  43, 189],\n",
       "          [  0,  47, 192],\n",
       "          ...,\n",
       "          [  0,  52, 196],\n",
       "          [  0,  37, 185],\n",
       "          [  0,  52, 196]],\n",
       "\n",
       "         [[  0,  56, 198],\n",
       "          [  0,  57, 198],\n",
       "          [  0,  64, 202],\n",
       "          ...,\n",
       "          [  0,  57, 198],\n",
       "          [  0,  65, 203],\n",
       "          [  0,  58, 199]],\n",
       "\n",
       "         [[  0,  56, 198],\n",
       "          [  0,  59, 200],\n",
       "          [  0,  67, 204],\n",
       "          ...,\n",
       "          [  0,  60, 200],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  49, 194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 123, 225],\n",
       "          [  0, 121, 225],\n",
       "          [  0, 123, 225],\n",
       "          ...,\n",
       "          [  0, 108, 221],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 108, 221]],\n",
       "\n",
       "         [[  0, 117, 224],\n",
       "          [  0, 112, 222],\n",
       "          [  0, 112, 222],\n",
       "          ...,\n",
       "          [  0, 100, 218],\n",
       "          [  0, 107, 220],\n",
       "          [  0, 108, 221]],\n",
       "\n",
       "         [[  0,  65, 203],\n",
       "          [  0,  81, 210],\n",
       "          [  0,  79, 210],\n",
       "          ...,\n",
       "          [  0,  60, 200],\n",
       "          [  0,  66, 203],\n",
       "          [  0,  72, 206]]],\n",
       "\n",
       "\n",
       "        [[[  0,  48, 193],\n",
       "          [  0,  44, 190],\n",
       "          [  0,  39, 186],\n",
       "          ...,\n",
       "          [  0,  44, 190],\n",
       "          [  0,  46, 192],\n",
       "          [ 46,  26, 174]],\n",
       "\n",
       "         [[  0,  58, 199],\n",
       "          [  0,  52, 196],\n",
       "          [  0,  49, 194],\n",
       "          ...,\n",
       "          [  0,  57, 198],\n",
       "          [  0,  69, 205],\n",
       "          [  0,  47, 192]],\n",
       "\n",
       "         [[  0,  51, 195],\n",
       "          [  0,  46, 192],\n",
       "          [  0,  55, 197],\n",
       "          ...,\n",
       "          [  0,  47, 192],\n",
       "          [  0,  61, 201],\n",
       "          [  0,  49, 194]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[  0, 112, 222],\n",
       "          [  0, 108, 221],\n",
       "          [  0, 108, 221],\n",
       "          ...,\n",
       "          [  0, 114, 222],\n",
       "          [  0, 115, 223],\n",
       "          [  0, 115, 223]],\n",
       "\n",
       "         [[  0, 100, 218],\n",
       "          [  0,  94, 216],\n",
       "          [  0,  91, 214],\n",
       "          ...,\n",
       "          [  0, 105, 220],\n",
       "          [  0, 105, 220],\n",
       "          [  0, 105, 220]],\n",
       "\n",
       "         [[  0,  72, 206],\n",
       "          [  0,  76, 208],\n",
       "          [  0,  82, 211],\n",
       "          ...,\n",
       "          [  0,  76, 208],\n",
       "          [  0,  70, 205],\n",
       "          [  0,  64, 202]]]]], dtype=uint8)"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=loader.data[\"train\"]\n",
    "print(X.shape)\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.3731861114501953, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.3597798347473145, 0.5]\n",
      "Time for 2 iterations: 1.8883638381958008\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 14.285714285714286% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 14.285714285714286, previous best: -1\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.3524065017700195, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.317732810974121, 0.5]\n",
      "Time for 4 iterations: 3.7334847450256348\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 42.857142857142854, previous best: 14.285714285714286\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.3191590309143066, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.27704119682312, 0.5]\n",
      "Time for 6 iterations: 5.590775966644287\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 42.857142857142854\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.3031136989593506, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.2046103477478027, 0.5]\n",
      "Time for 8 iterations: 7.322049856185913\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.2615392208099365, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.1557319164276123, 0.5]\n",
      "Time for 10 iterations: 8.883739948272705\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 10\n",
      "training loss:  [3.1557319164276123, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.1720733642578125, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.2048733234405518, 0.5]\n",
      "Time for 12 iterations: 10.489436864852905\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.182577133178711, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.099611520767212, 1.0]\n",
      "Time for 14 iterations: 12.090017795562744\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.0352959632873535, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.1596896648406982, 0.5]\n",
      "Time for 16 iterations: 13.669281721115112\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.012702703475952, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.785839796066284, 1.0]\n",
      "Time for 18 iterations: 15.390013933181763\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.0424132347106934, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.098444938659668, 0.5]\n",
      "Time for 20 iterations: 17.012267112731934\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 20\n",
      "training loss:  [3.098444938659668, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.7694618701934814, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.973909378051758, 1.0]\n",
      "Time for 22 iterations: 18.65250277519226\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.167964458465576, 0.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.7325546741485596, 1.0]\n",
      "Time for 24 iterations: 20.338565826416016\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.865133047103882, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.6919443607330322, 1.0]\n",
      "Time for 26 iterations: 22.13689684867859\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.6543073654174805, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.6561737060546875, 1.0]\n",
      "Time for 28 iterations: 23.905585050582886\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5629868507385254, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.6553375720977783, 1.0]\n",
      "Time for 30 iterations: 25.7307767868042\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 30\n",
      "training loss:  [2.6553375720977783, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5762012004852295, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.607478618621826, 1.0]\n",
      "Time for 32 iterations: 27.429067134857178\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 85.71428571428571, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5071985721588135, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5563180446624756, 1.0]\n",
      "Time for 34 iterations: 29.23309898376465\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 85.71428571428571\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.89102840423584, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.763331413269043, 1.0]\n",
      "Time for 36 iterations: 31.02425503730774\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.826216697692871, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.7564966678619385, 0.5]\n",
      "Time for 38 iterations: 32.6946759223938\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.630936861038208, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4742190837860107, 1.0]\n",
      "Time for 40 iterations: 34.37921094894409\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 40\n",
      "training loss:  [2.4742190837860107, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4748144149780273, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4149513244628906, 1.0]\n",
      "Time for 42 iterations: 36.11617588996887\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4112420082092285, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.407130718231201, 1.0]\n",
      "Time for 44 iterations: 37.796956062316895\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.39274001121521, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.7083842754364014, 1.0]\n",
      "Time for 46 iterations: 39.491798877716064\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 14.285714285714286% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.3595919609069824, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.353844165802002, 1.0]\n",
      "Time for 48 iterations: 41.31786489486694\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.3478803634643555, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.214674472808838, 1.0]\n",
      "Time for 50 iterations: 42.99206209182739\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 50\n",
      "training loss:  [2.214674472808838, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.710895538330078, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2769622802734375, 1.0]\n",
      "Time for 52 iterations: 44.88866591453552\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.464768409729004, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1566829681396484, 1.0]\n",
      "Time for 54 iterations: 46.67753791809082\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [3.536098003387451, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.453747510910034, 1.0]\n",
      "Time for 56 iterations: 48.33933091163635\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2912583351135254, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2809884548187256, 1.0]\n",
      "Time for 58 iterations: 50.11231279373169\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.262324571609497, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.34903883934021, 1.0]\n",
      "Time for 60 iterations: 51.93329191207886\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 60\n",
      "training loss:  [2.34903883934021, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.257171392440796, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.465092182159424, 1.0]\n",
      "Time for 62 iterations: 53.67765688896179\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2281253337860107, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5350136756896973, 1.0]\n",
      "Time for 64 iterations: 55.492319107055664\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.303058624267578, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2413554191589355, 1.0]\n",
      "Time for 66 iterations: 57.288594007492065\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.5814061164855957, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.3105437755584717, 1.0]\n",
      "Time for 68 iterations: 59.0597620010376\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.219395399093628, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1756856441497803, 1.0]\n",
      "Time for 70 iterations: 60.8927948474884\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 70\n",
      "training loss:  [2.1756856441497803, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4028172492980957, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.150829792022705, 1.0]\n",
      "Time for 72 iterations: 62.755878925323486\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.227158546447754, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1264946460723877, 1.0]\n",
      "Time for 74 iterations: 64.48518586158752\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2999908924102783, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.503462553024292, 0.5]\n",
      "Time for 76 iterations: 66.25471997261047\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1324758529663086, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1287343502044678, 1.0]\n",
      "Time for 78 iterations: 68.00813913345337\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.107809066772461, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1108856201171875, 1.0]\n",
      "Time for 80 iterations: 69.73098874092102\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 80\n",
      "training loss:  [2.1108856201171875, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1385746002197266, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.142900228500366, 1.0]\n",
      "Time for 82 iterations: 71.55268001556396\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.0786287784576416, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2086129188537598, 0.5]\n",
      "Time for 84 iterations: 73.33904385566711\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.208238124847412, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2116689682006836, 0.5]\n",
      "Time for 86 iterations: 75.19685697555542\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9852471351623535, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9542028903961182, 1.0]\n",
      "Time for 88 iterations: 77.06209588050842\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.0246565341949463, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.951313853263855, 1.0]\n",
      "Time for 90 iterations: 78.88965892791748\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 90\n",
      "training loss:  [1.951313853263855, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1364850997924805, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.0248606204986572, 1.0]\n",
      "Time for 92 iterations: 80.72329783439636\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.034008502960205, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.00105357170105, 1.0]\n",
      "Time for 94 iterations: 82.53589487075806\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.590341091156006, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8718211650848389, 1.0]\n",
      "Time for 96 iterations: 84.36827611923218\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.6181654930114746, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8911051750183105, 1.0]\n",
      "Time for 98 iterations: 86.14422297477722\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8800945281982422, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8061176538467407, 1.0]\n",
      "Time for 100 iterations: 87.93825006484985\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 100\n",
      "training loss:  [1.8061176538467407, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8779220581054688, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9718754291534424, 1.0]\n",
      "Time for 102 iterations: 89.70449113845825\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7637742757797241, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9498164653778076, 1.0]\n",
      "Time for 104 iterations: 91.4316918849945\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7777568101882935, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.941847324371338, 1.0]\n",
      "Time for 106 iterations: 93.15807604789734\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2363781929016113, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9987082481384277, 1.0]\n",
      "Time for 108 iterations: 94.8541488647461\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.012812376022339, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.981065034866333, 1.0]\n",
      "Time for 110 iterations: 96.78721189498901\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 110\n",
      "training loss:  [1.981065034866333, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.978676199913025, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8416707515716553, 1.0]\n",
      "Time for 112 iterations: 98.69233107566833\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8400859832763672, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9168577194213867, 1.0]\n",
      "Time for 114 iterations: 100.49265694618225\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.846832275390625, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.890852451324463, 1.0]\n",
      "Time for 116 iterations: 102.2422730922699\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.739525318145752, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8748892545700073, 1.0]\n",
      "Time for 118 iterations: 104.04355907440186\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7496991157531738, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7306281328201294, 1.0]\n",
      "Time for 120 iterations: 105.78191685676575\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 120\n",
      "training loss:  [1.7306281328201294, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6371787786483765, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8550857305526733, 1.0]\n",
      "Time for 122 iterations: 107.4793291091919\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8457660675048828, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2055399417877197, 0.5]\n",
      "Time for 124 iterations: 109.16724586486816\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9346857070922852, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7356584072113037, 1.0]\n",
      "Time for 126 iterations: 110.90984606742859\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.051698684692383, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.0886616706848145, 0.5]\n",
      "Time for 128 iterations: 112.72469782829285\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8175936937332153, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.0597434043884277, 0.5]\n",
      "Time for 130 iterations: 114.44229578971863\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 130\n",
      "training loss:  [2.0597434043884277, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.803344488143921, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.001988649368286, 0.5]\n",
      "Time for 132 iterations: 116.17002201080322\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.1233859062194824, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7933111190795898, 1.0]\n",
      "Time for 134 iterations: 117.87472796440125\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8885419368743896, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.838474154472351, 1.0]\n",
      "Time for 136 iterations: 119.84280395507812\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.745028018951416, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7460615634918213, 1.0]\n",
      "Time for 138 iterations: 121.5900309085846\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.4892027378082275, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6222785711288452, 1.0]\n",
      "Time for 140 iterations: 123.30879282951355\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 140\n",
      "training loss:  [1.6222785711288452, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.830476999282837, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7554659843444824, 1.0]\n",
      "Time for 142 iterations: 125.03921389579773\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8200786113739014, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.935825228691101, 0.5]\n",
      "Time for 144 iterations: 126.79845190048218\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7564669847488403, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.806714653968811, 1.0]\n",
      "Time for 146 iterations: 128.5987069606781\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.130174160003662, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7761850357055664, 1.0]\n",
      "Time for 148 iterations: 130.35575985908508\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6211127042770386, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6588553190231323, 1.0]\n",
      "Time for 150 iterations: 132.06225395202637\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 150\n",
      "training loss:  [1.6588553190231323, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7653292417526245, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7023509740829468, 1.0]\n",
      "Time for 152 iterations: 133.84742188453674\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.681045413017273, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6974793672561646, 1.0]\n",
      "Time for 154 iterations: 135.62936305999756\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.2428483963012695, 0.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9423162937164307, 1.0]\n",
      "Time for 156 iterations: 137.32962203025818\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6817753314971924, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9573864936828613, 1.0]\n",
      "Time for 158 iterations: 139.42779397964478\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.9687473773956299, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6241241693496704, 1.0]\n",
      "Time for 160 iterations: 141.2908480167389\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 160\n",
      "training loss:  [1.6241241693496704, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7330200672149658, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5155659914016724, 1.0]\n",
      "Time for 162 iterations: 143.02072882652283\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.676411747932434, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5874491930007935, 1.0]\n",
      "Time for 164 iterations: 144.7067699432373\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6778477430343628, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6166768074035645, 1.0]\n",
      "Time for 166 iterations: 146.4184868335724\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6533896923065186, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6579501628875732, 1.0]\n",
      "Time for 168 iterations: 148.15222692489624\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6116172075271606, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.64453125, 1.0]\n",
      "Time for 170 iterations: 149.82814502716064\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 170\n",
      "training loss:  [1.64453125, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.910485029220581, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7111603021621704, 1.0]\n",
      "Time for 172 iterations: 151.67766880989075\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7178785800933838, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5277305841445923, 1.0]\n",
      "Time for 174 iterations: 153.56890392303467\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4924185276031494, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6191784143447876, 1.0]\n",
      "Time for 176 iterations: 155.56195378303528\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6532446146011353, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.226534843444824, 0.5]\n",
      "Time for 178 iterations: 157.36478805541992\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5195475816726685, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6316967010498047, 1.0]\n",
      "Time for 180 iterations: 159.10226583480835\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 180\n",
      "training loss:  [1.6316967010498047, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7549445629119873, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.843123435974121, 0.5]\n",
      "Time for 182 iterations: 160.87653994560242\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5212926864624023, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7861371040344238, 0.5]\n",
      "Time for 184 iterations: 162.7718768119812\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.869842767715454, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5467578172683716, 1.0]\n",
      "Time for 186 iterations: 164.50351190567017\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6954538822174072, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8941895961761475, 0.5]\n",
      "Time for 188 iterations: 166.15308284759521\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4793651103973389, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7249343395233154, 1.0]\n",
      "Time for 190 iterations: 167.80888891220093\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 190\n",
      "training loss:  [1.7249343395233154, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.581849455833435, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5102323293685913, 1.0]\n",
      "Time for 192 iterations: 169.48773789405823\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4083727598190308, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8474845886230469, 0.5]\n",
      "Time for 194 iterations: 171.19126677513123\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5920811891555786, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8371143341064453, 0.5]\n",
      "Time for 196 iterations: 173.00957989692688\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.549768328666687, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.504942774772644, 1.0]\n",
      "Time for 198 iterations: 174.7777099609375\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4545713663101196, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6311922073364258, 1.0]\n",
      "Time for 200 iterations: 176.5373089313507\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 200\n",
      "training loss:  [1.6311922073364258, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.8959145545959473, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4325830936431885, 1.0]\n",
      "Time for 202 iterations: 178.37809109687805\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4923235177993774, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3133864402770996, 1.0]\n",
      "Time for 204 iterations: 180.1189341545105\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5366499423980713, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.371748685836792, 1.0]\n",
      "Time for 206 iterations: 181.82389211654663\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.141057014465332, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6913845539093018, 0.5]\n",
      "Time for 208 iterations: 183.58173489570618\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3642204999923706, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3526661396026611, 1.0]\n",
      "Time for 210 iterations: 185.43179988861084\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 210\n",
      "training loss:  [1.3526661396026611, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.495988368988037, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3181965351104736, 1.0]\n",
      "Time for 212 iterations: 187.21048402786255\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4616527557373047, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5665359497070312, 1.0]\n",
      "Time for 214 iterations: 188.93041706085205\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3915390968322754, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.3970184326171875, 0.0]\n",
      "Time for 216 iterations: 190.71044993400574\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.506140947341919, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3613433837890625, 1.0]\n",
      "Time for 218 iterations: 192.54514288902283\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2776886224746704, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2857671976089478, 1.0]\n",
      "Time for 220 iterations: 194.29375004768372\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 220\n",
      "training loss:  [1.2857671976089478, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3883846998214722, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2442626953125, 1.0]\n",
      "Time for 222 iterations: 196.14343190193176\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4655961990356445, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3669514656066895, 1.0]\n",
      "Time for 224 iterations: 197.9923059940338\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.282733678817749, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2886302471160889, 1.0]\n",
      "Time for 226 iterations: 199.8775029182434\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.225521206855774, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2298862934112549, 1.0]\n",
      "Time for 228 iterations: 201.74712300300598\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.413314938545227, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6846929788589478, 1.0]\n",
      "Time for 230 iterations: 203.49063396453857\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 230\n",
      "training loss:  [1.6846929788589478, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4951870441436768, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.343342661857605, 1.0]\n",
      "Time for 232 iterations: 205.247220993042\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 28.571428571428573% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [4.651847839355469, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.2696845531463623, 1.0]\n",
      "Time for 234 iterations: 207.04767084121704\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [2.03672456741333, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.574177622795105, 1.0]\n",
      "Time for 236 iterations: 208.97677898406982\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4868894815444946, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.469292163848877, 1.0]\n",
      "Time for 238 iterations: 210.82156610488892\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6219121217727661, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5988999605178833, 1.0]\n",
      "Time for 240 iterations: 212.6614909172058\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 240\n",
      "training loss:  [1.5988999605178833, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.680740475654602, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4651646614074707, 1.0]\n",
      "Time for 242 iterations: 214.50757002830505\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5078020095825195, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6143150329589844, 1.0]\n",
      "Time for 244 iterations: 216.3179268836975\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5881538391113281, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.518017292022705, 0.5]\n",
      "Time for 246 iterations: 218.55534291267395\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5046290159225464, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4146418571472168, 1.0]\n",
      "Time for 248 iterations: 220.28759503364563\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5768071413040161, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6493496894836426, 0.5]\n",
      "Time for 250 iterations: 221.98441791534424\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 250\n",
      "training loss:  [1.6493496894836426, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4310632944107056, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.647055983543396, 1.0]\n",
      "Time for 252 iterations: 223.62076091766357\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4575198888778687, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.552572250366211, 0.5]\n",
      "Time for 254 iterations: 225.35066509246826\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4692976474761963, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5372331142425537, 1.0]\n",
      "Time for 256 iterations: 227.047434091568\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.375286340713501, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4306061267852783, 1.0]\n",
      "Time for 258 iterations: 228.7814428806305\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3817996978759766, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.356141209602356, 1.0]\n",
      "Time for 260 iterations: 230.48452496528625\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 260\n",
      "training loss:  [1.356141209602356, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3446234464645386, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6091026067733765, 1.0]\n",
      "Time for 262 iterations: 232.12761092185974\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.357249140739441, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3834047317504883, 1.0]\n",
      "Time for 264 iterations: 233.81695890426636\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 100.0% 8 way one-shot learning accuracy \n",
      "\n",
      "Current best: 100.0, previous best: 100.0\n",
      "Saving weights to: data/model_weights.h5 \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.385636806488037, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4894720315933228, 1.0]\n",
      "Time for 266 iterations: 235.569837808609\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.348465919494629, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4264044761657715, 1.0]\n",
      "Time for 268 iterations: 237.19083976745605\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4351989030838013, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3694989681243896, 1.0]\n",
      "Time for 270 iterations: 238.84010696411133\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 42.857142857142854% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 270\n",
      "training loss:  [1.3694989681243896, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4276776313781738, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3232266902923584, 1.0]\n",
      "Time for 272 iterations: 240.5912368297577\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3150391578674316, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.6857271194458008, 0.5]\n",
      "Time for 274 iterations: 242.24079012870789\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3335767984390259, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7384114265441895, 0.5]\n",
      "Time for 276 iterations: 244.02248287200928\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.94010591506958, 0.5]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5571625232696533, 1.0]\n",
      "Time for 278 iterations: 245.8866240978241\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3087307214736938, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4496012926101685, 1.0]\n",
      "Time for 280 iterations: 247.71536016464233\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "iteration 280\n",
      "training loss:  [1.4496012926101685, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3304938077926636, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.7902522087097168, 0.5]\n",
      "Time for 282 iterations: 249.53976607322693\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 57.142857142857146% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3741741180419922, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.391269326210022, 1.0]\n",
      "Time for 284 iterations: 251.40435886383057\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3711193799972534, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.3610531091690063, 1.0]\n",
      "Time for 286 iterations: 253.15447306632996\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 71.42857142857143% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4252512454986572, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.5724034309387207, 1.0]\n",
      "Time for 288 iterations: 254.85774087905884\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 85.71428571428571% 8 way one-shot learning accuracy \n",
      "\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.290602207183838, 1.0]\n",
      "\n",
      " ------------- \n",
      "\n",
      "Loss: [1.4069607257843018, 1.0]\n",
      "Time for 290 iterations: 256.5833170413971\n",
      "Evaluating model on 7 random 8 way one-shot learning tasks ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_every = 1 # interval for evaluating on one-shot tasks\n",
    "loss_every = 10 # interval for printing loss (iterations)\n",
    "batch_size = 2\n",
    "n_iter = 1000\n",
    "N_way = 8 # how many classes for testing one-shot tasks>\n",
    "n_val = 7 # how many one-shot tasks to validate on?\n",
    "best = -1\n",
    "\n",
    "weights_path = os.path.join(data_path, \"model_weights.h5\")\n",
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "t_start = time.time()\n",
    "for i in range(1, n_iter):\n",
    "    (inputs,targets)=loader.get_batch(batch_size)\n",
    "    loss=model.train_on_batch(inputs,targets)\n",
    "    print(\"\\n ------------- \\n\")\n",
    "    print(\"Loss: {0}\".format(loss)) \n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"Time for {0} iterations: {1}\".format(i, time.time()-t_start))\n",
    "        val_acc = loader.test_oneshot(model,N_way,n_val,verbose=True)\n",
    "        if val_acc >= best:\n",
    "            print(\"Current best: {0}, previous best: {1}\".format(val_acc, best))\n",
    "            print(\"Saving weights to: {0} \\n\".format(weights_path))\n",
    "            model.save_weights(weights_path)\n",
    "            best=val_acc\n",
    "    \n",
    "    if i % loss_every == 0:\n",
    "        print(\"iteration\", i)\n",
    "        print(\"training loss: \", loss)\n",
    "\n",
    "        \n",
    "\n",
    "model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_neighbour_correct(pairs,targets):\n",
    "    \"\"\"returns 1 if nearest neighbour gets the correct answer for a one-shot task\n",
    "        given by (pairs, targets)\"\"\"\n",
    "    L2_distances = np.zeros_like(targets)\n",
    "    for i in range(len(targets)):\n",
    "        L2_distances[i] = np.sum(np.sqrt(pairs[0][i]**2 - pairs[1][i]**2))\n",
    "    if np.argmin(L2_distances) == np.argmax(targets):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_nn_accuracy(N_ways,n_trials,loader):\n",
    "    \"\"\"Returns accuracy of one shot \"\"\"\n",
    "    print(\"Evaluating nearest neighbour on {} unique {} way one-shot learning tasks ...\".format(n_trials,N_ways))\n",
    "\n",
    "    n_right = 0\n",
    "    \n",
    "    for i in range(n_trials):\n",
    "        pairs,targets = loader.make_oneshot_task(N_ways,\"val\")\n",
    "        correct = nearest_neighbour_correct(pairs,targets)\n",
    "        n_right += correct\n",
    "    return 100.0 * n_right / n_trials\n",
    "\n",
    "\n",
    "ways = np.arange(1, 8, 1)\n",
    "resume =  False\n",
    "val_accs, train_accs,nn_accs = [], [], []\n",
    "trials = 20\n",
    "for N in ways:\n",
    "    val_accs.append(loader.test_oneshot(model, N, trials, \"val\", verbose=True))\n",
    "    train_accs.append(loader.test_oneshot(model, N, trials, \"train\", verbose=True))\n",
    "    nn_accs.append(test_nn_accuracy(N,trials, loader))\n",
    "    \n",
    "#plot the accuracy vs num categories for each\n",
    "plt.plot(ways, val_accs, \"m\")\n",
    "plt.plot(ways, train_accs, \"y\")\n",
    "plt.plot(ways, nn_accs, \"c\")\n",
    "\n",
    "plt.plot(ways,100.0/ways,\"r\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35cd7540fa12df777e260f1a143c2189cfdfa0ea17d8fbbca8583add86510685"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
