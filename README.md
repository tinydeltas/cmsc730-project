# cmsc730-project

## Description 

## Getting started

1. Set up virtualenv using `setup.sh` or by running 

```
#!/bin/zsh

python3 -m venv venv 
source venv/bin/activate
pip install jupyter
ipython kernel install --name "local-venv" --user
python -m pip install -r requirements.txt
```

2. Run `pipeline.py`: 
## Directory overview 
- `data/`: Data for predefined gestures. 
    - `images`: The spectrogram images generated from the raw `.wav` files
    - `wav`: The raw `.wav` files of the eight predefined gestures, collected by the SEEED microphone mounted on a raspberry pi. 

- `pipeline/` 
    - `constants.py`: 
    
    `input_image_types`: Defines the input image (spectrogram) types we are interested in feeding to the ML model for training and validation. 
    
    `gestures`: labels for the pre-defined gestures (corresponding to the names of their respective folders in `data/images`)
    
    Other tweakable parameters: 
            `param_data_path`: Directory for output of spectrogram generation step. 
            **Default `./data/images/`**
            `param_dataset_path`: Directory for output of each run. 
            **Default `./tmp`.**
            `param_training_percentage`: Percentage of dataset for each gesture that will be allotted to the trainings set. 
            **Default 0.55.**
    
    - `gen_spectrograms.py`: Produces spectrograms from raw sound data. 
    
    - `gen_datasets.py`: Prepare the dataset for the model training step. Divides the spectrogram image data generated by `gen_spectrograms.py` into training and validation data sets. Selects `param_training_percentage * #_samples_per_gesture` samples for the training dataset at random; the rest comprise the validation data set. 
    
    - `ml_siamese.py`: Defines the `Fewshot` implementation. Skeleton code taken from https://github.com/akshaysharma096/Siamese-Networks and heavily modified for the purposes of this assignment.  

    - `pipeline.py`: Runs the overall pipeline. 


Temporary folders
- `tmp` Stores the dataset, ML models, and results for each run 
    - `YYYY-MM-DD HH-MM-SS`: Overall run folder, corresponding to each time `pipeline.py` is run. 
        - `models`
        - `results`: 

## Project roadmap 

### Milestone 1 (Predefined gestures)
- Data collection of pre-defined gestures: collected using SEEED microphone 
- Spectogram generation 
- Few-shot learning implementation 
#### Pre-defined gestures to verbal command mapping for Alexa/Google 

Lights
- Open ("Turn on the lights") 
- Close ("Turn off the lights")

Music
- Volume down (on music)
- Volume up (on music)

Calls
- No (hang up a call)

Misc 
- Tap (Check my calendar today)
- Cut (what's the weather)
- Switch (what's the time) 


### Milestone 2 (Real-time gesture detection and personalization)
- Gesture detection 
- Web application interface 
- ML experiments 
    - LSTM implementation 
    - MLP implementation 
    - Transfer learning implementation 
        - Dataset discovery
- Data collection with on-board microphone (PC or mobile)


